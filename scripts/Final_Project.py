#%%
# Import relevant libraries
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import openai
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt
print('Imported')

#%%
#1. Job Profiles

#1.1. Exploration

#Import merged jobs dataset
ESCO=pd.ExcelFile(r"..\datasets\Merged Jobs.xlsx")
jobs=pd.read_excel(ESCO, 'Merged Table')
print(jobs.head())
print(jobs.shape)

#Output number of skills for each job
skills_nr = jobs['occupations_en.preferredLabel'].value_counts()
print(skills_nr.min())
print(skills_nr.max())

#Visualize distribution of number of skills for each job
plt.boxplot(skills_nr)
plt.xlabel('Number of skills per job')
plt.title('Distribution of skills per job')
plt.show()

# Check for null values
jobs.isna().sum()

#%%
# 1.2. Preparation
# Normalize number of skills for further clustering improvement by capping the number of skills per job at 80
def limit_skills(skills_list):
    return skills_list.head(80)
jobs_capped= jobs.groupby('occupations_en.preferredLabel').apply(limit_skills).reset_index(drop=True)
print(jobs_capped.shape)

#Double check number of skills by job
skills_nr2 = jobs_capped['occupations_en.preferredLabel'].value_counts()
print(skills_nr2.min())
print(skills_nr2.max())

#Visualize distribution of number of skills for each job after capping
plt.boxplot(skills_nr2)
plt.xlabel('Number of skills per job')
plt.title('Distribution of skills per job')
plt.show()

# Rename and extract relevant columns
jobs_capped['Occupation']=jobs_capped['occupations_en.preferredLabel']
jobs_capped['Skill']=jobs_capped['skills_en.preferredLabel']
jobs_capped=jobs_capped[['Occupation','Skill']]
print(jobs_capped.head())
print(jobs_capped.shape)

# Remove spaces for multi word skills in order to reduce bias during vectorization
jobs_capped['Skill Vec']=jobs_capped['Skill'].str.replace(' ', '', regex=False)
print(jobs_capped['Skill Vec'].head(10))

#Join separate skills into 1 single String in order to form a valid input for TfidfVectorizer
jobs_grouped=jobs_capped.groupby('Occupation')['Skill Vec'].apply(lambda related_skills: ' '.join(related_skills)).reset_index()
print(jobs_grouped.head())
print(jobs_grouped.shape)

"""
Note: the following code was originally run in a Python notebook. In order to not execute the API calls at each compilation and overwrite 
exported CSV files, it has been commented out and is there only for transparency and logic understanding. 


#Generate descriptions for each job using opnan AI API. Descriptions are saved and exported after each batch of 50 rows. A try/catch
#statement has been build in in order to handle unexpected errors. 
import openai
import time

openai.api_key = 'KEY REMOVED'

def generate_description(job_title, skills):
    prompt = (
        f"Write a clear, concise job description for a {job_title}, requiring the following skills: {skills}, "
        f"so 16 year old students can understand what a professional in this job is doing on a daily basis. "
        f"Limit the response to 200 words."
    )
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=400,
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error on '{job_title}': {e}")
        return "ERROR: Failed to generate description."

jobs_grouped['Description'] = ""

for idx, row in jobs_grouped.iterrows():
    jobs_grouped.at[idx, 'Description'] = generate_description(row['Occupation'], row['Skill Vec'])
    time.sleep(1.2)  # Gentle sleep to avoid hitting rate limits

    if idx % 50 == 0 and idx > 0:
        jobs_grouped.to_csv("jobs_with_descriptions_partial.csv", index=False)
        print(f"Saved progress at row {idx}")

jobs_grouped.to_csv("jobs_with_descriptions.csv", index=False)
print("Finished and saved full dataset.")
"""

#Reading the csv generated by the API calls containing job descriptions
descriptions = pd.read_csv("../datasets/jobs_with_descriptions.csv")
print(descriptions.head())

#Adding to the job description column to the jobs_grouped dataframe
jobs_grouped['Description']=descriptions['Description']
jobs_grouped[['Occupation','Description']].head()

#Checking for jobs that failed to have descriptions generated
errors=descriptions[descriptions['Description'].str.contains("ERROR: Failed", na=False)]
print(errors[['Occupation','Description']])

#Manual generation of description for the 3 failed jobs
descriptions.loc[
    descriptions['Occupation'] == 'complementary therapist','Description'] = ("""A complementary therapist helps people feel better through natural, non-medical treatments.
    They use therapies like massage, aromatherapy, meditation, and other holistic techniques to reduce stress and improve overall well-being. 
    In their daily work, they listen to clients, understand their needs, and use various methods to promote relaxation and healing. 
    They often work in wellness centers or private practices, guiding individuals to live healthier lives by balancing mind and body. 
    Their role is supportive, helping to complement traditional medical treatments with natural care.""")

descriptions.loc[
    descriptions['Occupation'] == 'public administration manager','Description'] = ("""A public administration manager is responsible 
    for ensuring that government services and programs run smoothly. They work within public institutions to plan budgets, manage staff, 
    and develop policies that meet community needs. This role involves coordinating different departments, solving problems, and making 
    sure that public resources are used wisely. They communicate between government officials and the public, ensuring transparency 
    and efficiency. Strong organizational and leadership skills are essential in this job, which plays a key role in keeping essential 
    public services operating effectively.""")

descriptions.loc[
    descriptions['Occupation'] == 'rental service representative in other machinery, equipment and tangible goods','Description'] = (""""A rental 
    service representative in machinery, equipment, and tangible goods helps customers rent various types of equipment, such as construction machinery 
    or office tools. They explain rental terms, assist in choosing the right equipment, and manage the delivery, pickup, and billing processes. 
    This role requires clear communication, good organizational skills, and some technical knowledge about the products. Their job is to ensure 
    that customers understand their rental agreements and receive prompt help if issues arise, thereby keeping rental operations smooth and efficient 
    for both the business and its clients.""")

print(descriptions[descriptions['Description'].str.contains("ERROR: Failed", na=False)])

#%%
# 1.3. Vectorization

# Apply TfidfVectorizer in order to apply PCA analysis
vectorizer = TfidfVectorizer()
vec_jobs_skills = vectorizer.fit_transform(jobs_grouped['Skill Vec'])
print(vec_jobs_skills.shape)

#%%
# 1.4. PCA Dimensionality Reduction

# Apply PCA in order to reduce the high number of skills present in the dataset
reduced_vec_jobs_skills = PCA(n_components=0.95).fit_transform(vec_jobs_skills.toarray())
print(reduced_vec_jobs_skills.shape)
#%%
# 1.5. Domains of Interest Clustering

# 15 clusters simulation with K-means
clustered_jobs_15 = KMeans(n_clusters=15, random_state=10)
clusters_15 = clustered_jobs_15.fit_predict(reduced_vec_jobs_skills)
jobs_grouped['Cluster K-15'] = clusters_15

jobs_grouped['Cluster K-15'] = jobs_grouped['Cluster K-15'].astype(int)
ordered_clusters_1 = sorted(jobs_grouped['Cluster K-15'].unique())
plt.figure()
sns.countplot(data=jobs_grouped, x="Cluster K-15",order=ordered_clusters_1, palette="mako")
plt.title("Number of Occupations per Cluster after K-Means Clustering n=15")
plt.xlabel("Cluster")
plt.ylabel("Number of Occupations")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# 25 clusters simulation with K-means
clustered_jobs_25 = KMeans(n_clusters=25, random_state=10)
clusters_25 = clustered_jobs_25.fit_predict(reduced_vec_jobs_skills)
jobs_grouped['Cluster K-25'] = clusters_25

jobs_grouped['Cluster K-25'] = jobs_grouped['Cluster K-25'].astype(int)
ordered_clusters_2 = sorted(jobs_grouped['Cluster K-25'].unique())
plt.figure()
sns.countplot(data=jobs_grouped, x="Cluster K-25",order=ordered_clusters_2, palette="mako")
plt.title("Number of Occupations per Cluster after K-Means Clustering n=25")
plt.xlabel("Cluster")
plt.ylabel("Number of Occupations")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# 30 clusters simulation with K-means
clustered_jobs_30 = KMeans(n_clusters=30, random_state=10)
clusters_30 = clustered_jobs_30.fit_predict(reduced_vec_jobs_skills)
jobs_grouped['Cluster K-30'] = clusters_30

jobs_grouped['Cluster K-30'] = jobs_grouped['Cluster K-30'].astype(int)
ordered_clusters_3 = sorted(jobs_grouped['Cluster K-30'].unique())
plt.figure()
sns.countplot(data=jobs_grouped, x="Cluster K-30",order=ordered_clusters_3, palette="mako")
plt.title("Number of Occupations per Cluster after K-Means Clustering n=30")
plt.xlabel("Cluster")
plt.ylabel("Number of Occupations")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Agglomerative Clustering simulation for 25 clusters
agglomerated_jobs_25 = AgglomerativeClustering(n_clusters=25)
labels_25 = agglomerated_jobs_25.fit_predict(reduced_vec_jobs_skills)
jobs_grouped["Cluster Agg-25"] = labels_25

jobs_grouped['Cluster Agg_25'] = jobs_grouped['Cluster Agg-25'].astype(int)
ordered_clusters_4 = sorted(jobs_grouped['Cluster Agg-25'].unique())
plt.figure()
sns.countplot(data=jobs_grouped, x="Cluster Agg-25",order=ordered_clusters_4, palette="mako")
plt.title("Number of Occupations per Cluster after Agglomerative Clustering n=25")
plt.xlabel("Cluster")
plt.ylabel("Number of Occupations")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Agglomerative Clustering simulation for 35 clusters
agglomerated_jobs_35 = AgglomerativeClustering(n_clusters=35)
labels_35 = agglomerated_jobs_35.fit_predict(reduced_vec_jobs_skills)
jobs_grouped["Cluster Agg-35"] = labels_35

jobs_grouped['Cluster Agg_35'] = jobs_grouped['Cluster Agg-35'].astype(int)
ordered_clusters_5 = sorted(jobs_grouped['Cluster Agg-35'].unique())
plt.figure()
sns.countplot(data=jobs_grouped, x="Cluster Agg-35",order=ordered_clusters_5, palette="mako")
plt.title("Number of Occupations per Cluster after Agglomerative Clustering n=35")
plt.xlabel("Cluster")
plt.ylabel("Number of Occupations")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# 30 clusters simulation with K-means for non reduced data (not taking into consideration PCA)
clustered_jobs_30_bis = KMeans(n_clusters=30, random_state=10)
clusters_30_bis = clustered_jobs_30_bis.fit_predict(vec_jobs_skills)
jobs_grouped['Cluster K-30_Bis'] = clusters_30_bis

jobs_grouped['Cluster K-30_Bis'] = jobs_grouped['Cluster K-30_Bis'].astype(int)
ordered_clusters_6 = sorted(jobs_grouped['Cluster K-30_Bis'].unique())
plt.figure()
sns.countplot(data=jobs_grouped, x="Cluster K-30_Bis",order=ordered_clusters_6, palette="mako")
plt.title("Number of Occupations per Cluster after K-Means Clustering n=30 for non Reduced Features")
plt.xlabel("Cluster")
plt.ylabel("Number of Occupations")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Extract 10 top labels for each K-means for non reduced data cluster in order to surface more general area / domain of interest
def print_top_terms(kmeans_model, vectorizer, n_terms=10):
    terms = vectorizer.get_feature_names_out()
    centers = kmeans_model.cluster_centers_

    for point, centroid in enumerate(centers):
        top_term_indices = centroid.argsort()[::-1][:n_terms]
        top_terms = [terms[i] for i in top_term_indices]
        print(f"Cluster {point}: {', '.join(top_terms)}")

print_top_terms(kmeans_model=clustered_jobs_30_bis,vectorizer=vectorizer)

# Map clusters with their respective domain of interests. Domains are generated via chat GPT based on terms for each cluster.
domains = {
    0: "Environmental Compliance",
    1: "Construction",
    2: "Retail Management",
    3: "Educational Administration",
    4: "Footwear and Leather Production",
    5: "Mechanical Engineering",
    6: "Metal Manufacturing",
    7: "Textile Technology",
    8: "Leather Processing",
    9: "Social Work",
    10: "Animal Control and Security",
    11: "Food Manufacturing",
    12: "Public Administration",
    13: "Academic Research",
    14: "Healthcare",
    15: "Electromechanics",
    16: "Technical Arts Support",
    17: "Restoration and Craftsmanship",
    18: "International Trade Compliance",
    19: "Commodity Trading",
    20: "Customer Service",
    21: "Software Development",
    22: "Agriculture and Animal Husbandry",
    23: "Production Quality Control",
    24: "Financial Services",
    25: "Industrial Machinery Operation",
    26: "Data and IT Security",
    27: "Technical Support",
    28: "Teaching",
    29: "Logistics and Supply Chain"
}

# Generated domains are mapped with their respective clusters
jobs_grouped['Domain of Interest'] = jobs_grouped['Cluster K-30_Bis'].map(domains)
print(jobs_grouped[['Occupation','Domain of Interest']].head(20))
print(jobs_grouped[['Occupation','Domain of Interest']].tail(20))

"""
Note: commenting out this code snippet in order to not overwrite the stored file at each compilation

#Filter out and export jobs having the "Agriculture and Animal Husbandry" label as this cluster seems to imbalanced
imbalanced_cluster= jobs_grouped[jobs_grouped['Domain of Interest'] == 'Agriculture and Animal Husbandry']
imbalanced_cluster.to_csv("../datasets/misclassified_jobs.csv", index=False, quoting=1, encoding='utf-8')
print('Exported')
"""

#Reimport file after jobs having the "Agriculture and Animal Husbandry"  have been manually relabeled
relabeled_cluster=pd.read_csv("../datasets/Reclassified_jobs.csv")
print(relabeled_cluster.shape)
print(relabeled_cluster['Domain of Interest'].value_counts())
print(relabeled_cluster['Domain of Interest'].nunique())

#Remove previously mislabeled jobs and concatenate newly created dataset
filtered_jobs = jobs_grouped[jobs_grouped['Domain of Interest'] != 'Agriculture and Animal Husbandry']
updated_jobs_grouped = pd.concat([filtered_jobs, relabeled_cluster], ignore_index=True)
print(updated_jobs_grouped.shape)
print(updated_jobs_grouped['Domain of Interest'].value_counts())
print(updated_jobs_grouped['Domain of Interest'].nunique())

# Concatenate skills and area of interest for second vectorization
updated_jobs_grouped["Skill and Domain"] = updated_jobs_grouped["Skill Vec"] + " " + updated_jobs_grouped["Domain of Interest"]
print(updated_jobs_grouped['Skill and Domain'].head())

"""
Note: commenting out this code snippet in order to not overwrite the stored file at each compilation

#Export updated_jobs_grouped for usage in the Streamlit script. 
updated_jobs_grouped.to_csv("../datasets/updated_jobs_grouped.csv", index=False,encoding='utf-8')
"""

#%%
# 2. Student Profiles
# 2.1. Data Setup

#Extracting unique skills and domains to compute student profiles
ESCO=pd.ExcelFile(r"..\datasets\Merged Jobs.xlsx")
skills_tab=pd.read_excel(ESCO, 'Merged Table')
student_skills=skills_tab['skills_en.preferredLabel'].unique()
student_domains=updated_jobs_grouped['Domain of Interest'].unique()
print(student_skills[0:5])
print(student_domains[0:5])
print(student_skills.size)
print(student_domains.size)

#Compute student profiles dataset by randomly selecting skills and domains of interest. Those profiles will serve as test profiles later on. 
def create_profiles():
    skills = np.random.choice(student_skills, 5, replace=False)
    skill_vec = ' '.join(skill.replace(' ', '').lower() for skill in skills)
    domain = np.random.choice(student_domains)
    return {'Skill Vec': skill_vec, 'Domain of Interest': domain}

student_profiles = [create_profiles() for _ in range(20000)]
students = pd.DataFrame(student_profiles)

print(students.head())
print(students.shape)
print(students['Skill Vec'].nunique())

"""
Note: commenting out this code snippet in order to not overwrite the stored file at each compilation

#Export updated_jobs_grouped for usage in the Streamlit script. Commenting out once code has run
students.to_csv("../datasets/students.csv", index=False,encoding='utf-8')
"""